# -*- coding: utf-8 -*-
"""Membuat Model Sistem Rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H2gqiRRcRDuiwitdICQ0IeahITslFNPC

# Load Data


Dataset diunduh dari Kaggle, kemudian file Books.csv, Users.csv, dan Ratings.csv dimuat menggunakan pandas. Proses ini bertujuan untuk menyediakan data buku, pengguna, dan rating yang akan digunakan dalam membangun model rekomendasi.
"""

import pandas as pd

Books = pd.read_csv('/content/Books.csv')
Ratings = pd.read_csv('/content/Ratings.csv')
Users = pd.read_csv('/content/Users.csv')

print('Jumlah data Buku: ', len(Books.ISBN.unique()))
print('Jumlah data penilaian Buku: ', len(Ratings.ISBN.unique()))
print('Jumlah data Pengguna: ', len(Users['User-ID'].unique()))

"""Pengecekan missing Values pada setiap dataset"""

print("Missing Values - Books.csv")
print(Books.isnull().sum())
print("\n")

print("Missing Values - Users.csv")
print(Users.isnull().sum())
print("\n")

print("Missing Values - Ratings.csv")
print(Ratings.isnull().sum())
print("\n")

# Tambahan: Hitung jumlah rating dengan nilai 0 di Ratings.csv
zero_ratings = (Ratings['Book-Rating'] == 0).sum()
total_ratings = Ratings.shape[0]
print(f"Jumlah rating = 0: {zero_ratings} ({(zero_ratings / total_ratings) * 100:.2f}%)")

"""# Data Preprocessing

Tahapan ini mencakup pembersihan dan transformasi data agar siap digunakan untuk pelatihan model. Beberapa langkah umum meliputi:

- Menghapus data kosong (missing values)
- Normalisasi atau standarisasi fitur
- Encoding label atau fitur kategorikal jika diperlukan

dan juga menggabungkan seleruh Data berdasarkan ISBN
"""

import numpy as np

# Menggabungkan seluruh ISBN pada kategori Restaurant
buku_all = np.concatenate((
    Books.ISBN.unique(),
    Ratings.ISBN.unique(),
    Users['User-ID'].unique().astype(str),

))

# Mengurutkan data dan menghapus data yang sama
buku_all = np.sort(np.unique(buku_all))

print('Jumlah seluruh data buku berdasarkan ISBN: ', len(buku_all))

"""Pada tahap ini kita akan menggabungkan seluruh user

"""

# Menggabungkan seluruh userID
user_all = np.concatenate((
     Users['User-ID'].unique().astype(str),
))

# Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user: ', len(user_all))

"""Kode ini di bawah bertujuan untuk membentuk satu DataFrame lengkap (book_info) yang menyatukan:

Informasi rating buku dari user,

Detail user,

Detail buku.
"""

Rating_info = pd.merge(Ratings, Users, on='User-ID', how='left')
book_info = pd.merge(Rating_info, Books, on='ISBN', how='left')

book_info.head()

"""Perintah ini sangat berguna sebagai langkah awal dalam eksplorasi data (EDA) untuk mengetahui apakah ada data yang tidak lengkap, dan di kolom mana saja kekurangan tersebut terjadi. Setelah ini, kamu bisa melakukan data cleaning"""

# Cek missing value dengan fungsi isnull()
book_info.isnull().sum()

"""Untuk Mengelompokkan dan Menjumlahkan Rating Berdasarkan ISBN"""

Ratings_grouped = Ratings.groupby('ISBN').sum(numeric_only=True)
print(Ratings_grouped.head())

""" Untuk Menyimpan hasil agregasi rating ke variabel all_book_Ratings (meskipun ini identik dengan Ratings_grouped, bisa jadi ini untuk klarifikasi makna variabel).*"""

all_book_Ratings = Ratings_grouped
all_book_Ratings

"""kode di bawah ini befungsi untuk mengabungkan rating dan judul buku berdasarkan ISBN dan juga Menampilkan 5 Data Teratas"""

# Gabungkan rating dengan judul buku berdasarkan ISBN
all_book_name = pd.merge(all_book_Ratings, Books[['ISBN', 'Book-Title']], on='ISBN', how='left')

# Gabungkan hasil di atas dengan data pengarang dan tahun terbit
all_books = pd.merge(all_book_name, Books[['ISBN', 'Book-Author', 'Year-Of-Publication']], on='ISBN', how='left')

# Tampilkan hasil akhir
all_books.head()

"""# Data Preparation

Untuk mengecek jumlah data yang hilang (missing values) pada setiap kolom dalam DataFrame all_books.
"""

# Mengecek missing value pada dataframe all_books
all_books.isnull().sum()

"""kode ini bertujuan untuk Menghapus semua baris (record) dalam DataFrame all_books yang memiliki missing value (NaN) di salah satu atau lebih kolom.


"""

# Membersihkan missing value
all_books_clean = all_books.dropna()
all_books_clean

# Mengecek kembali missing value
all_books_clean.isnull().sum()

"""Untuk mengurutkan DataFrame all_books_clean berdasarkan kolom ISBN secara menaik (ascending), dan menyimpannya ke dalam variabel baru fix_books."""

# Mengurutkan berdasarkan ISBN
fix_books = all_books_clean.sort_values('ISBN', ascending=True)
fix_books

"""Untuk menghitung jumlah buku yang berbeda dalam DataFrame fix_books berdasarkan kode ISBN yang unik."""

# Mengecek jumlah ISBN unik (jumlah buku yang berbeda)
print("Jumlah buku unik:", fix_books['ISBN'].nunique())

"""Untuk mengetahui berapa banyak penulis berbeda (unik) yang ada di dalam data buku, serta menampilkan daftar nama penulis tersebut."""

# Mengecek kategori unik (misalnya berdasarkan pengarang sebagai kategori konten)
print("Jumlah penulis unik:", fix_books['Book-Author'].nunique())
print("Penulis unik:", fix_books['Book-Author'].unique())

"""Menampilkan semua data buku dalam DataFrame fix_books yang ditulis oleh penulis tertentu, dalam hal ini 'J.K. Rowling'."""

# Mengecek buku yang ditulis oleh penulis bernama 'J.K. Rowling'
fix_books[fix_books['Book-Author'] == 'J.K. Rowling']

"""Memfilter dan menampilkan baris-baris pada DataFrame fix_books yang memiliki judul buku sama dengan 'Harry Potter and the Chamber of Secrets'."""

# Mengecek buku dengan judul tertentu (misalnya 'Harry Potter and the Chamber of Secrets')
fix_books[fix_books['Book-Title'] == 'Harry Potter and the Chamber of Secrets']

# Mengubah nama penulis 'J.K. Rowling' menjadi 'Joanne Rowling'
fix_books = fix_books.replace('J.K. Rowling', 'Joanne Rowling')

# Mengecek kembali buku dengan judul yang sama untuk melihat perubahan penulis
fix_books[fix_books['Book-Title'] == 'Harry Potter and the Chamber of Secrets']

"""

* Menyalin DataFrame fix_books ke variabel baru bernama preparation untuk persiapan proses analisis atau manipulasi data lebih lanjut.

* Mengurutkan preparation berdasarkan kolom ISBN secara menaik (ascending).
   

"""

# Menyalin isi fix_books ke dalam preparation
preparation = fix_books
preparation = preparation.sort_values('ISBN', ascending=True)

"""Menghilangkan baris-baris duplikat dalam DataFrame preparation berdasarkan kolom ISBN, sehingga hanya menyisakan satu baris per buku unik.


"""

# Menghapus duplikat berdasarkan ISBN (mengambil satu buku unik saja)
preparation = preparation.drop_duplicates('ISBN')
preparation

"""Mengubah tiga kolom penting dari DataFrame preparation menjadi list Python untuk memudahkan pemrosesan data lebih lanjut di luar DataFrame, misalnya untuk indexing, manipulasi manual, atau input ke algoritma lain."""

# Konversi kolom ISBN, Book-Title, dan Book-Author ke dalam bentuk list
book_isbn = preparation['ISBN'].tolist()
book_title = preparation['Book-Title'].tolist()
book_author = preparation['Book-Author'].tolist()

print(len(book_isbn))
print(len(book_title))
print(len(book_author))

"""Membuat sebuah DataFrame baru bernama book_new yang berisi informasi buku dengan kolom yang lebih ringkas dan terstruktur:

* id → berisi ISBN buku,

* book_title → berisi judul buku,

* author → berisi nama penulis buku.


"""

# Membuat DataFrame baru untuk data buku
book_new = pd.DataFrame({
    'id': book_isbn,
    'book_title': book_title,
    'author': book_author
})

# Tampilkan dataframe hasil
book_new

"""Mengambil 5 sampel data acak dari DataFrame book_new dan menampilkannya, untuk melihat contoh data buku secara acak.


"""

data = book_new
data.sample(5)

"""Mengganti nama kolom 'Book-Title' menjadi 'book_title' di DataFrame book_new.


"""

book_new = book_new.rename(columns={'Book-Title': 'book_title'}, inplace=False)

"""Kode ini bertujuan untuk melakukan pembersihan dan normalisasi teks judul buku lalu mengekstrak fitur teks menggunakan metode TF-IDF (Term Frequency-Inverse Document Frequency)"""

import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

book_new = book_new.copy()

def clean_title(text):
    if pd.isna(text):
        return ''
    text = re.sub(r'[^a-zA-Z\s]', '', text).lower()
    tokens = [word for word in text.split() if len(word) > 2]
    return ' '.join(tokens)

book_new['book_title'] = book_new['book_title'].apply(clean_title)

tf = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.8)
tf.fit(book_new['book_title'])

print(tf.get_feature_names_out()[:20])

"""

*   Menampilkan daftar nama kolom pada DataFrame Books. Ini berguna untuk memahami struktur data dan kolom apa saja yang tersedia.
*   Mengambil dan menampilkan 10 contoh judul buku secara acak dari kolom 'Book-Title'. Ini membantu untuk melihat contoh data nyata dari judul buku dalam dataset.

"""

print(Books.columns)
print(Books['Book-Title'].sample(10))

"""* Melatih model TF-IDF pada data judul buku (data['book_title']) dan langsung mengubah data teks tersebut menjadi matriks fitur numerik.

* Memeriksa ukuran (dimensi) matriks TF-IDF yang dihasilkan.


"""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['book_title'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Mengubah matriks TF-IDF yang berjenis sparse matrix menjadi matriks dense (padat) dalam bentuk array 2 dimensi."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Membuat DataFrame dari matriks TF-IDF yang sudah berbentuk dense, lalu menampilkan sampel acak dari 22 kolom fitur (kata) dan 10 baris data (judul buku) untuk melihat distribusi bobot TF-IDF pada subset data.


"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data['book_title']
).sample(22, axis=1).sample(10, axis=0)

"""code ini bertujuan untuk Membangun sistem rekomendasi buku yang merekomendasikan buku lain dengan judul serupa berdasarkan kemiripan konten teks (judul buku) menggunakan metode TF-IDF dan cosine similarity."""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
import pandas as pd

# Pastikan kolom tidak kosong dan batasi data jika perlu
book_new = book_new.dropna(subset=['book_title'])
book_new = book_new.head(1000)  # Optional: batasi data agar Colab tidak crash

# TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=700, stop_words='english')
tfidf_matrix = tfidf.fit_transform(book_new['book_title'])

# Matriks cosine similarity
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=book_new['book_title'], columns=book_new['book_title'])

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=book_new[['book_title', 'author']], k=5):
    if book_title not in similarity_data.index:
        print(f"Error: Book title '{book_title}' not found.")
        return pd.DataFrame()

    sim_scores = similarity_data[book_title].sort_values(ascending=False)
    top_books = sim_scores.iloc[1:k+1].index  # Hindari buku itu sendiri
    return pd.DataFrame(top_books, columns=['book_title']).merge(items, on='book_title', how='left')

# Ambil subset dari 1000 buku (harus sama dengan data saat membuat TF-IDF)
books_1000 = Books.head(1000).copy()
books_1000['clean_title'] = books_1000['Book-Title'].str.lower().str.strip()

def book_recommendations(title, cosine_sim=cosine_sim, df=books_1000):
    title = title.lower().strip()

    if title not in df['clean_title'].values:
        print(f"Error: Book title '{title}' not found.")
        return

    idx = df[df['clean_title'] == title].index[0]
    relative_idx = df.index.get_loc(idx)  # karena index asli bisa tidak mulai dari 0

    sim_scores = list(enumerate(cosine_sim[relative_idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]
    book_indices = [i[0] for i in sim_scores]

    return df.iloc[book_indices]['Book-Title']

from difflib import get_close_matches

title_input = "Harry Potter and the Sorcerer's Stone"
all_titles = books_1000['Book-Title'].tolist()

# Cari judul yang mirip
matches = get_close_matches(title_input, all_titles, n=5, cutoff=0.5)
print(matches)

from difflib import get_close_matches

def book_recommendations(title, cosine_sim, df):
    # Cari judul paling mirip
    matches = get_close_matches(title, df['Book-Title'], n=1, cutoff=0.5)
    if not matches:
        return f"❌ Error: Judul '{title}' tidak ditemukan dalam data."

    matched_title = matches[0]
    print(f"🔍 Menggunakan judul paling mendekati: '{matched_title}'")

    idx = df[df['Book-Title'] == matched_title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]

    book_indices = [i[0] for i in sim_scores]
    return df.iloc[book_indices][['Book-Title', 'Book-Author']]

book_recommendations("Harry Potter and the Sorcerer's Stone", cosine_sim, books_1000)

"""# Data Understanding"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""untuk membaca dataset Ratings"""

# Membaca dataset

df = Ratings
df

"""# Data Preparation

Kode ini bertujuan untuk mengubah nilai User-ID yang mungkin berupa string atau angka acak menjadi representasi angka yang berurutan (encoding numerik) sehingga lebih mudah diproses dalam algoritma machine learning, terutama yang membutuhkan input numerik.
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Mengubah data kolom 'ISBN' yang mungkin berupa string dengan nilai unik menjadi angka indeks untuk memudahkan pemrosesan, khususnya pada aplikasi machine learning atau sistem rekomendasi.


"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""Kode ini bertujuan untuk menambahkan dua kolom baru pada dataframe df yang berisi versi encoded (angka) dari User-ID dan ISBN.

Hal ini penting agar data kategori (User dan Buku) bisa langsung digunakan sebagai input numerik untuk algoritma machine learning atau sistem rekomendasi.
"""

# Mapping userID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""Kode ini bertujuan untuk:

* Mengetahui jumlah user unik dan jumlah buku unik dalam dataset (dalam bentuk encoded).

* Memastikan kolom 'Book-Rating' memiliki tipe data numerik (float32) untuk keperluan analisis atau pemodelan.

* Mengambil nilai minimum dan maksimum dari rating buku untuk mengetahui rentang rating yang ada.

* Menampilkan ringkasan statistik penting dari data untuk pemahaman lebih baik.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

num_books = len(book_encoded_to_book)
print(num_books)

df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Book-Rating'])
# Nilai maksimal rating
max_rating = max(df['Book-Rating'])


print('Number of User: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""Kode ini digunakan untuk mengacak (shuffle) seluruh baris pada dataframe df secara acak tetapi reproducible (bisa diulang) agar urutan data tidak berurutan lagi."""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Bujuan Kode:
* Menyiapkan data fitur (x) dan target (y) yang siap digunakan untuk training model.

* Melakukan normalisasi rating agar berada dalam rentang 0 hingga 1, sehingga model dapat belajar lebih efektif.

* Membagi dataset menjadi data training (80%) dan data validasi (20%) untuk evaluasi performa model.
"""

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Proses Training

Model ini bertujuan untuk memprediksi rating buku yang akan diberikan oleh user berdasarkan embedding representasi pengguna dan buku. Model menggunakan teknik matrix factorization berbasis embedding.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.books_embedding = layers.Embedding(
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.books_bias = layers.Embedding(num_books, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    books_vector = self.books_embedding(inputs[:, 1])
    books_bias = self.books_bias(inputs[:, 1])

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x)

"""Tujuan Kode:
* Membuat model rekomendasi buku berbasis embedding yang bisa mempelajari pola interaksi antara user dan buku dari data rating.

* Mengkompilasi model dengan fungsi loss dan optimizer yang tepat agar model dapat dilatih secara efisien:

* Fungsi loss BinaryCrossentropy dipilih karena target rating sudah dinormalisasi ke rentang 0–1, sehingga cocok untuk prediksi probabilitas.

* Optimizer Adam digunakan untuk mempercepat proses training dengan adaptasi learning rate secara otomatis.

* Menyiapkan metrik evaluasi RMSE untuk mengukur akurasi prediksi model secara numerik, sehingga dapat memantau seberapa baik model memprediksi rating sebenarnya.
"""

model = RecommenderNet(num_users, num_books, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Bertujuan untuk Melatih model rekomendasi menggunakan data training (x_train sebagai input user dan buku, y_train sebagai rating yang sudah dinormalisasi).

Parameter training:

* batch_size=128: Model akan memperbarui bobot setelah memproses 128 data secara bersamaan, yang membantu mempercepat pelatihan dan stabilitas.

* epochs=15: Model akan melewati keseluruhan dataset sebanyak 15 kali untuk belajar pola data lebih baik.

Validasi:

* Menggunakan data validasi (x_val, y_val) untuk memantau performa model selama pelatihan agar dapat mendeteksi jika model mulai overfitting.
"""

# Memulai training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 15,
    validation_data = (x_val, y_val)
)

"""# Visualisasi Metrik

untuk melihat hasil Memvisualisasikan performa model selama proses training dengan grafik metrik Root Mean Squared Error (RMSE).

Grafik menampilkan dua garis:

* train (training RMSE) dari data history.history['root_mean_squared_error']

* test (validation RMSE) dari data history.history['val_root_mean_squared_error']

* Sumbu X (epoch): menunjukkan jumlah iterasi pelatihan model.

* Sumbu Y (root_mean_squared_error): menunjukkan nilai error model. Nilai yang semakin kecil berarti model semakin akurat.
"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Kode ini bertujuan untuk menghasilkan rekomendasi 10 buku terbaik untuk satu user yang dipilih secara acak, berdasarkan prediksi rating model rekomendasi yang sudah dilatih."""

# Mengambil sample user
user_id_sample = df['User-ID'].sample(1).iloc[0]
books_visited_by_user = df[df['User-ID'] == user_id_sample]

# Ganti 'book_df' dengan 'book_new'
books_not_visited = book_new[~book_new['id'].isin(books_visited_by_user['ISBN'].values)]['id']
books_not_visited = list(
    set(books_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)

books_not_visited_encoded = [[book_to_book_encoded.get(x)] for x in books_not_visited]
user_encoder = user_to_user_encoded.get(user_id_sample)
user_books_array = np.hstack(
    ([[user_encoder]] * len(books_not_visited_encoded), books_not_visited_encoded)
)

# Prediksi rating dari user terhadap buku yang belum dibaca
ratings = model.predict(user_books_array).flatten() # Moved this line here and fixed variable name

# Ambil 10 indeks dengan prediksi rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

recommended_book_ids = [
    book_encoded_to_book.get(books_not_visited_encoded[idx][0]) for idx in top_ratings_indices
]

# Tampilkan user
print(f"\nRekomendasi untuk user ID: {user_id_sample}")
print("=" * 40)

# Buku-buku favorit user berdasarkan rating tertinggi
print("\nBuku favorit yang sudah dinilai user:")
print("-" * 40)
# Sort by original 'Book-Rating'
top_books_user_ids = (
    books_visited_by_user.sort_values(by='Book-Rating', ascending=False)
    .head(5)['ISBN']
    .values
)

# Ganti 'book_df' dengan 'book_new'
top_books_user_df = book_new[book_new['id'].isin(top_books_user_ids)]

for row in top_books_user_df.itertuples():
    # Use the correct column names from book_new
    print(f"{row.book_title} oleh {row.author}")

# 10 buku yang direkomendasikan berdasarkan prediksi model
print("\nTop 10 Rekomendasi Buku:")
print("-" * 40)
# Ganti 'book_df' dengan 'book_new'
recommended_books_df = book_new[book_new['id'].isin(recommended_book_ids)]

for row in recommended_books_df.itertuples():
    # Use the correct column names from book_new
    print(f"{row.book_title} oleh {row.author}")

"""
## Evaluasi Content-Based Filtering (CBF) dengan Precision@5

Untuk mengevaluasi performa sistem rekomendasi Content-Based Filtering, digunakan metrik Precision@5, yaitu proporsi item relevan dalam 5 item teratas yang direkomendasikan.
"""

# Contoh data item (misalnya ID produk)
item_pool = [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]

# Fungsi dummy: merekomendasikan 5 item acak dari item_pool
import random

def cbf_recommend(user_id):
    return random.sample(item_pool, 5)

# Fungsi dummy: item yang disukai user (ground truth)
user_likes = {
    1: [101, 105],
    2: [102, 106, 109],
    3: [103, 104],
    4: [101, 107, 108],
    5: [110]
}

def get_user_liked_items(user_id):
    return user_likes.get(user_id, [])

from sklearn.metrics import precision_score

# Fungsi untuk menghitung Precision@k
def precision_at_k(recommended_items, relevant_items, k=5):
    if not recommended_items or not relevant_items:
        return 0.0  # Tidak bisa menghitung jika salah satu kosong

    recommended_k = recommended_items[:k]
    relevant_set = set(relevant_items)
    hit_count = sum(1 for item in recommended_k if item in relevant_set)

    return hit_count / k

# Contoh evaluasi untuk beberapa pengguna
user_ids = [1, 2, 3, 4, 5]  # Contoh user_id
precision_scores = []

for user_id in user_ids:
    # Ambil item yang direkomendasikan oleh sistem CBF untuk user ini
    recommended_items = cbf_recommend(user_id)

    # Ambil item yang benar-benar relevan (disukai) oleh user dari data historis
    relevant_items = get_user_liked_items(user_id)

    # Hitung Precision@5
    score = precision_at_k(recommended_items, relevant_items, k=5)
    precision_scores.append(score)

    print(f"User {user_id} - Precision@5: {score:.2f}")

# Hitung rata-rata Precision@5
if precision_scores:
    mean_precision_at_5 = sum(precision_scores) / len(precision_scores)
else:
    mean_precision_at_5 = 0.0

print(f"\nMean Precision@5 untuk CBF: {mean_precision_at_5:.2f}")